{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dbe358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\hung0\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\hung0\\AppData\\Local\\Temp\\ipykernel_22412\\4134282230.py\", line 8, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\hung0\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import ml_collections\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import  Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import emoji\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re,string, nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d3b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config():\n",
    "    cfg_dictionary = {\n",
    "        \"data_path\": \"../../data/data.csv\",\n",
    "        \"model_path\": \"/kaggle/working/bert_model.h5\",\n",
    "        \"model_type\": \"transformer\",\n",
    "\n",
    "        \"test_size\": 0.1,\n",
    "        \"validation_size\":0.2,\n",
    "        \"train_batch_size\": 32,\n",
    "        \"eval_batch_size\": 32,\n",
    "\n",
    "        \"epochs\": 5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"lr\": 3e-5,\n",
    "        \"num_warmup_steps\": 10,\n",
    "\n",
    "        \"max_length\": 128,\n",
    "        \"random_seed\": 42,\n",
    "        \"num_labels\": 3,\n",
    "        \"model_checkpoint\":\"roberta-base\",\n",
    "    }\n",
    "    cfg = ml_collections.FrozenConfigDict(cfg_dictionary)\n",
    "\n",
    "    return cfg\n",
    "cfg = model_config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a0db16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hung0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\hung0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import emoji\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import STOPWORDS as WC_STOPWORDS\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1) Tải tài nguyên NLTK\n",
    "# ==============================\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stopwords: KHÔNG bỏ từ phủ định\n",
    "CUSTOM_STOPS = set(WC_STOPWORDS) - {\n",
    "    \"not\", \"no\", \"nor\", \"against\", \"ain\", \"aren\", \"couldn\", \"didn\", \"doesn\",\n",
    "    \"hadn\", \"hasn\", \"haven\", \"isn\", \"mightn\", \"mustn\", \"needn\", \"shan\",\n",
    "    \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\"\n",
    "}\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2) Regex compile trước cho nhanh\n",
    "# ==============================\n",
    "RE_HTML = re.compile(r\"<.*?>\")\n",
    "RE_URL = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "RE_MENTION = re.compile(r\"@\\w+\")\n",
    "RE_HASH = re.compile(r\"#\\w+\")\n",
    "# Giữ chữ, số, khoảng trắng và ký hiệu tài chính: $ % + - . , /\n",
    "RE_KEEP = re.compile(r\"[^A-Za-z0-9\\$\\%\\+\\-\\,\\./\\s]\")\n",
    "RE_MULTIWS = re.compile(r\"\\s+\")\n",
    "\n",
    "# ==============================\n",
    "# 3) Tiền xử lý văn bản\n",
    "# ==============================\n",
    "def expand_contractions(text: str) -> str:\n",
    "    \"\"\"Mở rộng các contractions phổ biến trong tiếng Anh.\"\"\"\n",
    "    t = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    t = re.sub(r\"can\\'t\", \"can not\", t)\n",
    "    t = re.sub(r\"n\\'t\", \" not\", t)\n",
    "    t = re.sub(r\"\\'re\", \" are\", t)\n",
    "    t = re.sub(r\"\\'s\", \" is\", t)\n",
    "    t = re.sub(r\"\\'d\", \" would\", t)\n",
    "    t = re.sub(r\"\\'ll\", \" will\", t)\n",
    "    t = re.sub(r\"\\'t\", \" not\", t)\n",
    "    t = re.sub(r\"\\'ve\", \" have\", t)\n",
    "    t = re.sub(r\"\\'m\", \" am\", t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def preprocess_text_fin(\n",
    "    text: str,\n",
    "    *,\n",
    "    use_lemma: bool = True,\n",
    "    remove_emoji: bool = False,\n",
    "    normalize_currency: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Tiền xử lý văn bản: bỏ HTML/URL/mention/hashtag, mở rộng contractions, chuẩn hóa ký hiệu tiền tệ, loại bỏ ký tự không cần thiết, lemmatize & bỏ stopwords (giữ từ phủ định).\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "\n",
    "    t = str(text).lower()\n",
    "\n",
    "    # 1) Bỏ HTML, URL, mention/hashtag\n",
    "    t = RE_HTML.sub(\" \", t)\n",
    "    t = RE_URL.sub(\" \", t)\n",
    "    t = RE_MENTION.sub(\" \", t)\n",
    "    t = RE_HASH.sub(\" \", t)\n",
    "\n",
    "    # 2) Mở rộng contractions\n",
    "    t = expand_contractions(t)\n",
    "\n",
    "    # 3) Chuẩn hóa tiền tệ thành token (tùy chọn)\n",
    "    if normalize_currency:\n",
    "        t = t.replace(\"$\", \" <currency> \")\n",
    "\n",
    "    # 4) Emoji/Non-ascii (tùy chọn)\n",
    "    if remove_emoji:\n",
    "        t = emoji.replace_emoji(t, replace=\" \")\n",
    "\n",
    "    # 5) Lọc ký tự nhưng GIỮ số & % + - . , /\n",
    "    t = RE_KEEP.sub(\" \", t)\n",
    "    t = RE_MULTIWS.sub(\" \", t).strip()\n",
    "\n",
    "    # 6) Lemmatization + bỏ stopwords (giữ từ phủ định)\n",
    "    if use_lemma:\n",
    "        tokens = []\n",
    "        for w in t.split():\n",
    "            if w in CUSTOM_STOPS:\n",
    "                continue\n",
    "            tokens.append(lemmatizer.lemmatize(w))\n",
    "        t = \" \".join(tokens)\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c8da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_csv(csv_file: str, cfg=None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Nhãn số -> cột 'labels'\n",
    "    df[\"labels\"] = LabelEncoder().fit_transform(df[\"Sentiment\"])\n",
    "\n",
    "    # Loại trùng câu\n",
    "    df.drop_duplicates(subset=[\"Sentence\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    # Chọn logic: nếu KHÔNG dùng transformer thì áp dụng preprocess_text_fin\n",
    "    use_transformer = (cfg is not None and getattr(cfg, \"model_type\", \"\") == \"transformer\")\n",
    "    if not use_transformer:\n",
    "        df[\"Sentence\"] = df[\"Sentence\"].astype(str).apply(preprocess_text_fin)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0afb42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_csv(cfg.data_path)\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(df[\"Sentence\"]),np.array(df[\"labels\"]), test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e23c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_IDF Model: Train features shape:(3991, 9933) and Test features shape:(1331, 9933)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(use_idf=True, tokenizer=word_tokenize,min_df=0.00002,max_df=0.70)\n",
    "X_train_tf = tfidf.fit_transform(X_train.astype('U'))\n",
    "X_test_tf = tfidf.transform(X_test.astype('U'))\n",
    "\n",
    "print(f\"TF_IDF Model: Train features shape:{X_train_tf.shape} and Test features shape:{X_test_tf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d70658cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:09<00:17,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8620\n",
      "[LightGBM] [Info] Number of data points in the train set: 3991, number of used features: 425\n",
      "[LightGBM] [Info] Start training from score -2.235013\n",
      "[LightGBM] [Info] Start training from score -0.604717\n",
      "[LightGBM] [Info] Start training from score -1.059064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:12<00:00,  8.06s/it]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "xgb = XGBClassifier(eval_metric=\"mlogloss\",random_state=42)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "svc = SVC(random_state=42)\n",
    "nb = MultinomialNB()\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "clfs = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"Gradient Boosting\":gb,\n",
    "    \"AdaBoost\": ada,\n",
    "    \"LightGBM\": lgb,\n",
    "    \"XGBoost\": xgb,\n",
    "    \"Decision Tree\":dt,\n",
    "    \"Support Vector Machine\":svc,\n",
    "    \"Naive Bayes\": nb,\n",
    "    \"Multilayer Perceptron\":mlp\n",
    "}\n",
    "\n",
    "def fit_model(clf,x_train,y_train,x_test, y_test):\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    return accuracy\n",
    "\n",
    "accuracys = []\n",
    "\n",
    "for name,clf in tqdm(clfs.items()):\n",
    "    curr_acc = fit_model(clf,X_train_tf,y_train,X_test_tf,y_test)\n",
    "    accuracys.append(curr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19e8b330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Accuracy Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.733283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.732532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.729527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.728775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.717506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>0.711495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.691961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.676935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.622840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Models  Accuracy Scores\n",
       "0           Random Forest         0.733283\n",
       "4                 XGBoost         0.732532\n",
       "1       Gradient Boosting         0.729527\n",
       "6  Support Vector Machine         0.728775\n",
       "3                LightGBM         0.717506\n",
       "8   Multilayer Perceptron         0.711495\n",
       "5           Decision Tree         0.691961\n",
       "7             Naive Bayes         0.676935\n",
       "2                AdaBoost         0.622840"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_df = pd.DataFrame({\"Models\":clfs.keys(),\"Accuracy Scores\":accuracys}).sort_values('Accuracy Scores',ascending=False)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0793a454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest ===\n",
      "Accuracy       : 0.7333\n",
      "F1-macro       : 0.5810\n",
      "F1-weighted    : 0.7020\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.14      0.23       165\n",
      "           1       0.75      0.92      0.83       698\n",
      "           2       0.70      0.66      0.68       468\n",
      "\n",
      "    accuracy                           0.73      1331\n",
      "   macro avg       0.73      0.57      0.58      1331\n",
      "weighted avg       0.73      0.73      0.70      1331\n",
      "\n",
      "ROC-AUC macro  : 0.8649\n",
      "\n",
      "=== Gradient Boosting ===\n",
      "Accuracy       : 0.7295\n",
      "F1-macro       : 0.6244\n",
      "F1-weighted    : 0.7072\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.28      0.41       165\n",
      "           1       0.72      0.94      0.81       698\n",
      "           2       0.75      0.58      0.65       468\n",
      "\n",
      "    accuracy                           0.73      1331\n",
      "   macro avg       0.74      0.60      0.62      1331\n",
      "weighted avg       0.73      0.73      0.71      1331\n",
      "\n",
      "ROC-AUC macro  : 0.8505\n",
      "\n",
      "=== AdaBoost ===\n",
      "Accuracy       : 0.6228\n",
      "F1-macro       : 0.4024\n",
      "F1-weighted    : 0.5531\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.01       165\n",
      "           1       0.63      0.96      0.76       698\n",
      "           2       0.59      0.34      0.44       468\n",
      "\n",
      "    accuracy                           0.62      1331\n",
      "   macro avg       0.57      0.44      0.40      1331\n",
      "weighted avg       0.60      0.62      0.55      1331\n",
      "\n",
      "ROC-AUC macro  : 0.7370\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8620\n",
      "[LightGBM] [Info] Number of data points in the train set: 3991, number of used features: 425\n",
      "[LightGBM] [Info] Start training from score -2.235013\n",
      "[LightGBM] [Info] Start training from score -0.604717\n",
      "[LightGBM] [Info] Start training from score -1.059064\n",
      "\n",
      "=== LightGBM ===\n",
      "Accuracy       : 0.7175\n",
      "F1-macro       : 0.6120\n",
      "F1-weighted    : 0.7012\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.28      0.37       165\n",
      "           1       0.75      0.89      0.81       698\n",
      "           2       0.68      0.62      0.65       468\n",
      "\n",
      "    accuracy                           0.72      1331\n",
      "   macro avg       0.66      0.60      0.61      1331\n",
      "weighted avg       0.70      0.72      0.70      1331\n",
      "\n",
      "ROC-AUC macro  : 0.8307\n",
      "\n",
      "=== XGBoost ===\n",
      "Accuracy       : 0.7325\n",
      "F1-macro       : 0.6303\n",
      "F1-weighted    : 0.7155\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.29      0.40       165\n",
      "           1       0.75      0.90      0.82       698\n",
      "           2       0.72      0.63      0.68       468\n",
      "\n",
      "    accuracy                           0.73      1331\n",
      "   macro avg       0.70      0.61      0.63      1331\n",
      "weighted avg       0.72      0.73      0.72      1331\n",
      "\n",
      "ROC-AUC macro  : 0.8610\n",
      "\n",
      "=== Decision Tree ===\n",
      "Accuracy       : 0.6920\n",
      "F1-macro       : 0.6051\n",
      "F1-weighted    : 0.6874\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.34      0.38       165\n",
      "           1       0.78      0.80      0.79       698\n",
      "           2       0.64      0.66      0.65       468\n",
      "\n",
      "    accuracy                           0.69      1331\n",
      "   macro avg       0.62      0.60      0.61      1331\n",
      "weighted avg       0.68      0.69      0.69      1331\n",
      "\n",
      "ROC-AUC macro  : 0.7126\n",
      "\n",
      "=== Support Vector Machine ===\n",
      "Accuracy       : 0.7288\n",
      "F1-macro       : 0.5748\n",
      "F1-weighted    : 0.6949\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.13      0.24       165\n",
      "           1       0.74      0.93      0.82       698\n",
      "           2       0.70      0.64      0.67       468\n",
      "\n",
      "    accuracy                           0.73      1331\n",
      "   macro avg       0.81      0.57      0.57      1331\n",
      "weighted avg       0.76      0.73      0.69      1331\n",
      "\n",
      "ROC-AUC macro  : 0.8714\n",
      "\n",
      "=== Naive Bayes ===\n",
      "Accuracy       : 0.6769\n",
      "F1-macro       : 0.4892\n",
      "F1-weighted    : 0.6289\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.05      0.10       165\n",
      "           1       0.68      0.93      0.79       698\n",
      "           2       0.65      0.51      0.57       468\n",
      "\n",
      "    accuracy                           0.68      1331\n",
      "   macro avg       0.78      0.50      0.49      1331\n",
      "weighted avg       0.71      0.68      0.63      1331\n",
      "\n",
      "ROC-AUC macro  : 0.8183\n",
      "\n",
      "=== Multilayer Perceptron ===\n",
      "Accuracy       : 0.7115\n",
      "F1-macro       : 0.6424\n",
      "F1-weighted    : 0.7085\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.42      0.46       165\n",
      "           1       0.78      0.79      0.78       698\n",
      "           2       0.67      0.69      0.68       468\n",
      "\n",
      "    accuracy                           0.71      1331\n",
      "   macro avg       0.65      0.63      0.64      1331\n",
      "weighted avg       0.71      0.71      0.71      1331\n",
      "\n",
      "ROC-AUC macro  : 0.8473\n",
      "\n",
      "=== Tổng hợp ===\n",
      "                 model  accuracy  f1_macro  f1_weighted  roc_auc_macro\n",
      " Multilayer Perceptron  0.711495  0.642390     0.708450       0.847257\n",
      "               XGBoost  0.732532  0.630330     0.715512       0.860963\n",
      "     Gradient Boosting  0.729527  0.624417     0.707227       0.850492\n",
      "              LightGBM  0.717506  0.612033     0.701232       0.830704\n",
      "         Decision Tree  0.691961  0.605099     0.687362       0.712553\n",
      "         Random Forest  0.733283  0.580977     0.702047       0.864904\n",
      "Support Vector Machine  0.728775  0.574840     0.694944       0.871440\n",
      "           Naive Bayes  0.676935  0.489170     0.628889       0.818254\n",
      "              AdaBoost  0.622840  0.402408     0.553102       0.737007\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def _maybe_proba_or_decision(clf, X):\n",
    "    \"\"\"Trả về scores xác suất/decision nếu có, else None.\"\"\"\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        return clf.predict_proba(X)\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        s = clf.decision_function(X)\n",
    "        # Decision có thể 1D với binary; chuyển thành 2 cột\n",
    "        if s.ndim == 1:\n",
    "            s = np.vstack([-s, s]).T\n",
    "        return s\n",
    "    return None\n",
    "\n",
    "def plot_confusion(y_true, y_pred, labels=None, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    plt.figure(figsize=(5.2, 4.5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(name, clf, Xtr, ytr, Xte, yte, label_names=None, show_plot=True):\n",
    "    \"\"\"Fit + in ra báo cáo & trả về metrics tổng hợp.\"\"\"\n",
    "    clf.fit(Xtr, ytr)\n",
    "    y_pred = clf.predict(Xte)\n",
    "\n",
    "    acc = accuracy_score(yte, y_pred)\n",
    "    f1_macro = f1_score(yte, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(yte, y_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy       : {acc:.4f}\")\n",
    "    print(f\"F1-macro       : {f1_macro:.4f}\")\n",
    "    print(f\"F1-weighted    : {f1_weighted:.4f}\")\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(yte, y_pred, target_names=label_names) if label_names is not None\n",
    "          else classification_report(yte, y_pred))\n",
    "\n",
    "    # Confusion matrix (plot)\n",
    "    if show_plot:\n",
    "        labels = list(range(len(np.unique(yte)))) if label_names is None else label_names\n",
    "        plot_confusion(yte, y_pred, labels=labels, title=f\"Confusion Matrix – {name}\")\n",
    "\n",
    "    # ROC-AUC macro (OVR) nếu có score\n",
    "    auc_macro = None\n",
    "    scores = _maybe_proba_or_decision(clf, Xte)\n",
    "    if scores is not None:\n",
    "        classes = np.unique(yte)\n",
    "        Y_true_bin = label_binarize(yte, classes=classes)\n",
    "        # Nếu scores shape không khớp số lớp, bỏ qua AUC\n",
    "        if scores.shape[1] == Y_true_bin.shape[1]:\n",
    "            try:\n",
    "                auc_macro = roc_auc_score(Y_true_bin, scores, average=\"macro\", multi_class=\"ovr\")\n",
    "                print(f\"ROC-AUC macro  : {auc_macro:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"(Bỏ qua ROC-AUC: {e})\")\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"roc_auc_macro\": auc_macro\n",
    "    }\n",
    "\n",
    "# ==== Chạy đánh giá cho tất cả models trong clfs ====\n",
    "results = []\n",
    "label_names = None  # hoặc: [\"negative\",\"neutral\",\"positive\"] nếu anh có mapping\n",
    "for name, clf in clfs.items():\n",
    "    res = evaluate_model(name, clf, X_train_tf, y_train, X_test_tf, y_test,\n",
    "                         label_names=label_names, show_plot=False)  # đặt True nếu muốn vẽ heatmap từng model\n",
    "    results.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"f1_macro\", ascending=False)\n",
    "print(\"\\n=== Tổng hợp ===\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
